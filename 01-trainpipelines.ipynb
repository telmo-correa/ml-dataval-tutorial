{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27871529-5ca7-4c7a-9dd9-8f1ce476908e",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](https://raw.githubusercontent.com/wandb/wandb/508982e50e82c54cbf0dd464a9959fee0e1740ad/.github/wb-logo-lightbg.png)\n",
    "<!--- @wandbcode{dataval-course-01} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac45bf86-8c9b-4750-bbc5-d948116fd821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataval.dataset import WeatherDataset\n",
    "from dataval.train import CatBoostTrainer\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from wandb.integration.catboost import WandbCallback\n",
    "\n",
    "os.environ[\"WANDB_QUIET\"] = \"true\" # Let's keep the output clean\n",
    "\n",
    "# Let's start a new W&B run to track our work\n",
    "run = wandb.init(project=\"ml-dataval-tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46faeb0-1dd6-43c1-92e2-2fb28158c45a",
   "metadata": {},
   "source": [
    "# Training Pipelines\n",
    "\n",
    "In this notebook, we load the weather dataset (described in https://arxiv.org/pdf/2107.07455.pdf). We split the dataset into weekly partitions. Then we create 2 pipelines: one that trains a model on the first week & deploys on all following weeks, and another that continually trains & deploys on each consecutive pair of weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c6600-0dc3-4619-b094-03f0785be53f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "ds = WeatherDataset(os.path.join(os.getcwd(), \"canonical-paritioned-dataset\"), sample_frac=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4257cc4-a48e-4de0-b77c-a262c21ad763",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train-Once\n",
    "\n",
    "We use a catboost model, optimizing for RMSE of weather prediction. We use off-the-shelf models/parameters (no ensembling or uncertainty estimation to keep things simple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a066b24-37ee-470b-92de-41b57a3627ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_iter = True\n",
    "test_mses = {}\n",
    "train_mse = None\n",
    "\n",
    "for train_df, test_df in ds.iterate():\n",
    "    X_train, y_train = ds.split_feature_label(train_df)\n",
    "    \n",
    "    if first_iter:\n",
    "        catboost_hparams = {\"depth\": 5, \"iterations\": 250, \"learning_rate\": 0.03, \"loss_function\": \"RMSE\"}\n",
    "        # Let's log the hyperparameters to W&B\n",
    "        wandb.config.update(catboost_hparams)\n",
    "        t = CatBoostTrainer(catboost_hparams)\n",
    "        print(f\"Training for {ds.get_partition_key(train_df)}...\")\n",
    "        # We'll pass in the W&B callback to log metrics\n",
    "        t.fit(X_train, y_train, verbose=100, callbacks=[WandbCallback()])\n",
    "        first_iter = False\n",
    "\n",
    "        train_mse = t.score(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    X_test, y_test = ds.split_feature_label(test_df)\n",
    "    test_mses[ds.get_partition_key(test_df)] = t.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be226066-c7d5-4559-8833-ea95653bde73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(test_mses.keys(), [train_mse] * len(test_mses.keys()), label=\"train\")\n",
    "plt.plot(test_mses.keys(), test_mses.values(), label=\"test\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"MSE over week (no retraining)\")\n",
    "plt.legend()\n",
    "# Let's log our plot to W&B so that we can refer to it in reports and dashboards\n",
    "wandb.log({\"MSE over week (no retraining)\": wandb.Image(plt)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ec3cc-d259-4353-8fe3-320b9698332f",
   "metadata": {},
   "source": [
    "Wow, it looks like the MSE gets significantly worse as we deploy over time! Makes sense, as the seasons change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d8762b-11d1-46a1-9dc7-cd65b7e576bd",
   "metadata": {},
   "source": [
    "## Continual-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2102fc7e-369e-4aab-9299-a8509fe53524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_mses = {}\n",
    "train_mses = {}\n",
    "\n",
    "for train_df, test_df in ds.iterate():\n",
    "    X_train, y_train = ds.split_feature_label(train_df)\n",
    "    \n",
    "    catboost_hparams = {\"depth\": 5, \"iterations\": 250, \"learning_rate\": 0.03, \"loss_function\": \"RMSE\"}\n",
    "    continual_t = CatBoostTrainer(catboost_hparams)\n",
    "    continual_t.fit(X_train, y_train, verbose=False)\n",
    "    train_mses[ds.get_partition_key(train_df)] = continual_t.score(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    X_test, y_test = ds.split_feature_label(test_df)\n",
    "    test_mses[ds.get_partition_key(test_df)] = continual_t.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7054579-b4de-4d27-bc99-8f42b8790419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(train_mses.keys(), train_mses.values(), label=\"train\")\n",
    "plt.plot(test_mses.keys(), test_mses.values(), label=\"test\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"MSE over week (with retraining)\")\n",
    "plt.legend()\n",
    "# Let's log our plot to W&B so that we can refer to it in reports and dashboards\n",
    "wandb.log({\"MSE over week (with retraining)\": wandb.Image(plt)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b7323-aa5c-4b38-8feb-b0eb8f43481d",
   "metadata": {},
   "source": [
    "The MSEs improve here, because we're retraining on fresh data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225550a7-07a3-4236-9170-aa269d44f663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "continual_t.get_feature_importance().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a4c8ab-eb5e-463d-a2ae-bd8075bdb69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can finish now the W&B run\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229143f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
